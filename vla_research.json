[
  {
    "id": 1,
    "title": "GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems",
    "year": "21 Jan 2020",
    "summary": "In vision-based object classification systems imaging sensors perceive the environment and machine learning is then used to detect and classify objects for decision-making purposes; e.g., to maneuver an automated vehicle around an obstacle or to raise an alarm to indicate the presence of an intruder in surveillance settings. In this work we demonstrate how the perception domain can be remotely and unobtrusively exploited to enable an attacker to create spurious objects or alter an existing object.",
    "sources": [
      {
        "type": "paper",
        "title": "GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems",
        "url": "https://arxiv.org/abs/2001.07792v3"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2001.07792v3"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/Harry1993/GhostImage"
      },
      {
        "type": "article",
        "title": "Phantom of the ADAS",
        "url": "https://www.nassiben.com/phantoms"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Camera Sensor Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 2,
    "title": "Adversarial Imaging Pipelines",
    "year": "07 Feb 2021",
    "summary": "Adversarial attacks play an essential role in understanding deep neural network predictions and improving their robustness. Existing attack methods aim to deceive convolutional neural network (CNN)-based classifiers by manipulating RGB images that are fed directly to the classifiers.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Imaging Pipelines",
        "url": "https://arxiv.org/abs/2102.03728v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2102.03728v2"
      },
      {
        "type": "website",
        "title": "Adversarial Imaging Pipelines project page",
        "url": "https://light.princeton.edu/publication/adversarial-pipelines/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Camera ISP Manipulation",
      "Adversarial Example Generation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 3,
    "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
    "year": "14 Apr 2021",
    "summary": "To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
        "url": "https://arxiv.org/abs/2104.06728v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2104.06728v2"
      },
      {
        "type": "journal",
        "title": "IEICE Transactions publication",
        "url": "https://ieice.org/en_transactions/information/10.1587/transinf.2022MUL0001/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Adversarial Patch",
      "Sticker Attack",
      "Traffic Sign Spoofing"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 4,
    "title": "DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems",
    "year": "07 Oct 2021",
    "summary": "Depth estimation-based obstacle avoidance has been widely adopted by autonomous systems (drones and vehicles) for safety purpose. It normally relies on a stereo camera to automatically detect obstacles and make flying/driving decisions, e.g., stopping several meters ahead of the obstacle in the path or moving away from the detected obstacle.",
    "sources": [
      {
        "type": "paper",
        "title": "DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems",
        "url": "https://arxiv.org/abs/2110.03154v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2110.03154v1"
      },
      {
        "type": "article",
        "title": "Security Analysis of Camera-LiDAR Fusion",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/hallyburton"
      },
      {
        "type": "website",
        "title": "DoubleStar project site",
        "url": "https://fakedepth.github.io/"
      },
      {
        "type": "talk",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/zhou-ce"
      },
      {
        "type": "pdf",
        "title": "USENIX Security 2022 paper",
        "url": "https://www.usenix.org/system/files/sec22-zhou-ce.pdf"
      },
      {
        "type": "article",
        "title": "On the Realism of LiDAR Spoofing Attacks",
        "url": "https://www.ndss-symposium.org/ndss-paper/on-the-realism-of-lidar-spoofing-attacks/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Injection",
      "Depth Estimation",
      "Drone Safety"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 5,
    "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
    "year": "06 Apr 2022",
    "summary": "Traffic light recognition is essential for fully autonomous driving in urban areas. In this paper, we investigate the feasibility of fooling traffic light recognition mechanisms by shedding laser interference on the camera.",
    "sources": [
      {
        "type": "paper",
        "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
        "url": "https://arxiv.org/abs/2204.02675v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2204.02675v1"
      },
      {
        "type": "slides",
        "title": "Rolling Colors slides",
        "url": "https://cyansec.com/files/slides/22SEC_RollingColors-slides.pdf"
      },
      {
        "type": "paper",
        "title": "Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs",
        "url": "https://proceedings.mlr.press/v189/hu23b.html"
      },
      {
        "type": "pdf",
        "title": "Adversarial Laser Spot PDF",
        "url": "https://proceedings.mlr.press/v189/hu23b/hu23b.pdf"
      },
      {
        "type": "pdf",
        "title": "USENIX Security 2022 Summer paper",
        "url": "https://www.usenix.org/system/files/sec22summer_yan.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Injection",
      "Traffic Light Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 6,
    "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
    "year": "13 Mar 2023",
    "summary": "Contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. However, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of targeted data poisoning and backdoor attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
        "url": "https://arxiv.org/abs/2303.06854v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.06854v2"
      }
    ],
    "tags": [
      "Defense",
      "Data Poisoning Mitigation",
      "Backdoor Defense",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 7,
    "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
    "year": "22 Mar 2023",
    "summary": "Adversarial attacks can mislead deep learning models to make false predictions by implanting small perturbations to the original input that are imperceptible to the human eye, which poses a huge security threat to the computer vision systems based on deep learning. Physical adversarial attacks, which is more realistic, as the perturbation is introduced to the input before it is being captured and converted to a binary image inside the vision system, when compared to digital adversarial attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
        "url": "https://arxiv.org/abs/2303.12249v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.12249v1"
      },
      {
        "type": "code",
        "title": "RFLA reflected light attack code",
        "url": "https://github.com/winterwindwang/RFLA"
      }
    ],
    "tags": [
      "Survey",
      "Physical Adversarial",
      "Optical Attack",
      "Threat Modeling"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 8,
    "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
    "year": "22 Oct 2023",
    "summary": "Despite the outstanding performance of deep neural networks, they are vulnerable to adversarial attacks. While there are many invisible attacks in the digital domain, most physical world adversarial attacks are visible.",
    "sources": [
      {
        "type": "paper",
        "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
        "url": "https://arxiv.org/abs/2311.16118v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16118v1"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Dazzle",
      "Sensor Jamming",
      "Rolling Shutter Exploit"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 9,
    "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
    "year": "05 Feb 2024",
    "summary": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts.",
    "sources": [
      {
        "type": "paper",
        "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
        "url": "https://arxiv.org/abs/2402.06659v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.06659v2"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/umd-huang-lab/VLM-Poisoning"
      },
      {
        "type": "code",
        "title": "ShadowAttack code",
        "url": "https://github.com/hncszya/ShadowAttack"
      }
    ],
    "tags": [
      "Data Poisoning",
      "Vision-Language Models",
      "Misinformation",
      "Stealth Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 10,
    "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
    "year": "15 Feb 2024",
    "summary": "In this work, we highlight vulnerabilities in robotic systems integrating large language models (LLMs) and vision-language models (VLMs) due to input modality sensitivities. While LLM/VLM-controlled robots show impressive performance across various tasks, their reliability under slight input variations remains underexplored yet critical.",
    "sources": [
      {
        "type": "paper",
        "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
        "url": "https://arxiv.org/abs/2402.10340v5"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.10340v5"
      }
    ],
    "tags": [
      "Robotics",
      "Input Perturbation",
      "Vision-Language Models",
      "Reliability"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 11,
    "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
    "year": "30 Apr 2024",
    "summary": "Backdoor attacks have been well-studied in visible light object detection (VLOD) in recent years. However, VLOD can not effectively work in dark and temperature-sensitive scenarios.",
    "sources": [
      {
        "type": "paper",
        "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
        "url": "https://arxiv.org/abs/2404.19417v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2404.19417v1"
      }
    ],
    "tags": [
      "Physical Backdoor",
      "Thermal Imaging",
      "Temperature Trigger",
      "Object Detection"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 12,
    "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
    "year": "16 May 2024",
    "summary": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2405.09981v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.09981v1"
      },
      {
        "type": "openreview",
        "title": "OpenReview discussion",
        "url": "https://openreview.net/forum?id=Fz0uYu6bZn"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Visual Grounding",
      "Adversarial Perturbation",
      "Multimodal Attacks"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 13,
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "year": "17 May 2024",
    "summary": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language models (VLMs) stand at the forefront of this advancement, offering innovative ways to combine visual and textual data for enhanced understanding and interaction.",
    "sources": [
      {
        "type": "paper",
        "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
        "url": "https://arxiv.org/abs/2405.10529v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.10529v2"
      },
      {
        "type": "openreview",
        "title": "OpenReview discussion",
        "url": "https://openreview.net/forum?id=2r8n6kKNEXNa"
      }
    ],
    "tags": [
      "Defense",
      "Adversarial Patch Mitigation",
      "Prompt Injection Defense",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 14,
    "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
    "year": "10 Jul 2024",
    "summary": "Camera-based computer vision is essential to autonomous vehicle's perception. This paper presents an attack that uses light-emitting diodes and exploits the camera's rolling shutter effect to create adversarial stripes in the captured images to mislead traffic sign recognition.",
    "sources": [
      {
        "type": "paper",
        "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
        "url": "https://arxiv.org/abs/2407.07510v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.07510v1"
      },
      {
        "type": "pdf",
        "title": "GhostStripe MobiSys PDF",
        "url": "https://tanrui.github.io/pub/GhostStripe-MobiSys.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Injection",
      "Traffic Sign Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 15,
    "title": "Prompt Injection Attacks on Large Language Models in Oncology",
    "year": "23 Jul 2024",
    "summary": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters.",
    "sources": [
      {
        "type": "paper",
        "title": "Prompt Injection Attacks on Large Language Models in Oncology",
        "url": "https://arxiv.org/abs/2407.18981v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.18981v1"
      },
      {
        "type": "journal",
        "title": "Nature Communications article",
        "url": "https://www.nature.com/articles/s41467-024-55631-x"
      }
    ],
    "tags": [
      "Prompt Injection",
      "Healthcare AI",
      "Vision-Language Models",
      "Safety Risk"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 16,
    "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
    "year": "07 Aug 2024",
    "summary": "We explore visual prompt injection (VPI) that maliciously exploits the ability of large vision-language models (LVLMs) to follow instructions drawn onto the input image. We propose a new VPI method, \"goal hijacking via visual prompt injection\" (GHVPI), that swaps the execution task of LVLMs from an original task to an alternative task designated by an attacker.",
    "sources": [
      {
        "type": "paper",
        "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
        "url": "https://arxiv.org/abs/2408.03554v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2408.03554v1"
      },
      {
        "type": "blog",
        "title": "GPT-4 vision prompt injection overview",
        "url": "https://blog.roboflow.com/gpt-4-vision-prompt-injection/"
      },
      {
        "type": "blog",
        "title": "Lakera guide to visual prompt injections",
        "url": "https://www.lakera.ai/blog/visual-prompt-injections"
      }
    ],
    "tags": [
      "Visual Prompt Injection",
      "Goal Hijacking",
      "Vision-Language Models",
      "Instruction Following Exploit"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 17,
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "year": "28 Sep 2024",
    "summary": "The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation.",
    "sources": [
      {
        "type": "paper",
        "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
        "url": "https://arxiv.org/abs/2409.19232v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2409.19232v1"
      }
    ],
    "tags": [
      "Backdoor Attack",
      "Vision-Language Models",
      "Image Captioning",
      "Trigger Phrases"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 18,
    "title": "Attacking Vision-Language Computer Agents via Pop-ups",
    "year": "04 Nov 2024",
    "summary": "Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear.",
    "sources": [
      {
        "type": "paper",
        "title": "Attacking Vision-Language Computer Agents via Pop-ups",
        "url": "https://arxiv.org/abs/2411.02391v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.02391v2"
      },
      {
        "type": "code",
        "title": "PopupAttack code",
        "url": "https://github.com/SALT-NLP/PopupAttack"
      }
    ],
    "tags": [
      "UI Manipulation",
      "Vision-Language Agents",
      "Adversarial Pop-ups",
      "Task Hijacking"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 19,
    "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
    "year": "18 Nov 2024",
    "summary": "Robotic manipulation in the physical world is increasingly empowered by large language models (LLMs) and vision-language models (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities.",
    "sources": [
      {
        "type": "paper",
        "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
        "url": "https://arxiv.org/abs/2411.11683v5"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.11683v5"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://trojanrobot.github.io"
      }
    ],
    "tags": [
      "Physical Backdoor",
      "Robotic Manipulation",
      "Vision-Language Models",
      "Module Poisoning"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 20,
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "year": "18 Nov 2024",
    "summary": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. Despite their significant capabilities, VLA models introduce new attack surfaces.",
    "sources": [
      {
        "type": "paper",
        "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
        "url": "https://arxiv.org/abs/2411.13587v4"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.13587v4"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/William-wAng618/roboticAttack"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://vlaattacker.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Adversarial Patch",
      "Robotic Control",
      "Trajectory Manipulation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 21,
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "year": "03 Jun 2025",
    "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs).",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Attacks on Robotic Vision Language Action Models",
        "url": "https://arxiv.org/abs/2506.03350v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.03350v1"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/eliotjones1/robogcg"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Jailbreaking",
      "Robotic Control Takeover",
      "Prompt-Based Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 22,
    "title": "Jailbreaking LLM-Controlled Robots",
    "year": "17 Oct 2024",
    "summary": "The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails.",
    "sources": [
      {
        "type": "paper",
        "title": "Jailbreaking LLM-Controlled Robots",
        "url": "https://arxiv.org/abs/2410.13691v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2410.13691v2"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://robopair.org"
      },
      {
        "type": "article",
        "title": "Wired coverage of robo jailbreaks",
        "url": "https://www.wired.com/story/researchers-llm-ai-robot-violence"
      }
    ],
    "tags": [
      "Jailbreaking",
      "Robotics",
      "Safety Vulnerability",
      "Prompt-Based Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 23,
    "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
    "year": "01 Jun 2025",
    "summary": "Projector-based adversarial attack aims to project carefully designed light patterns (i.e., adversarial projections) onto scenes to deceive deep image classifiers. It has potential applications in privacy protection and the development of more robust classifiers.",
    "sources": [
      {
        "type": "paper",
        "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
        "url": "https://arxiv.org/abs/2506.00978v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.00978v2"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/ZhanLiQxQ/CAPAA"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Multi-Classifier",
      "Stealth Perturbation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 24,
    "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
    "year": "17 Sep 2025",
    "summary": "Security researchers document how the Unitree G1 humanoid can be compromised through a BLE provisioning command injection that yields root access, aided by shared AES keys across devices. They further expose weaknesses in the robot's FMX encryption and persistent telemetry exfiltration, demonstrating how an onboard cybersecurity AI can escalate from reconnaissance to active offensive operations against external targets.",
    "sources": [
      {
        "type": "paper",
        "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
        "url": "https://arxiv.org/abs/2509.14139v3"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2509.14139v3"
      }
    ],
    "tags": [
      "Humanoid Robot Security",
      "Cybersecurity AI",
      "Robotic Control",
      "Autonomous Systems"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 25,
    "title": "The Cybersecurity of a Humanoid Robot",
    "year": "16 Sep 2025",
    "summary": "Provides a comprehensive penetration test of a production humanoid platform, cataloging firmware, wireless, and actuator vulnerabilities that enable persistent root access and remote control.",
    "sources": [
      {
        "type": "paper",
        "title": "The Cybersecurity of a Humanoid Robot",
        "url": "https://arxiv.org/abs/2509.14096"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2509.14096"
      }
    ],
    "tags": [
      "Humanoid Robot Security",
      "Vulnerability Assessment",
      "BLE Exploitation",
      "Remote Control"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 26,
    "title": "Robot Hacking Manual (RHM)",
    "year": "09 Mar 2022",
    "summary": "Compiles offensive security techniques for industrial and service robots, covering attack surfaces from network middleware to safety controllers with reproducible lab setups.",
    "sources": [
      {
        "type": "paper",
        "title": "Robot Hacking Manual (RHM)",
        "url": "https://arxiv.org/abs/2203.04765"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.04765"
      }
    ],
    "tags": [
      "Robotics Security",
      "Offensive Security",
      "Industrial Control",
      "Hands-On Labs"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 27,
    "title": "Offensive Robot Cybersecurity",
    "year": "23 Jun 2025",
    "summary": "Details a red-teaming methodology for robotic platforms that couples adversarial simulation, exploit development, and mitigations tailored to autonomous systems.",
    "sources": [
      {
        "type": "paper",
        "title": "Offensive Robot Cybersecurity",
        "url": "https://arxiv.org/abs/2506.15343"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.15343"
      }
    ],
    "tags": [
      "Robotics Security",
      "Red Team",
      "Penetration Testing",
      "Autonomous Systems"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 28,
    "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem",
    "year": "27 Aug 2025",
    "summary": "Systematizes known vulnerabilities across humanoid robot hardware, software supply chains, and cloud services, prioritizing mitigations for safety-critical deployments.",
    "sources": [
      {
        "type": "paper",
        "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem",
        "url": "https://arxiv.org/abs/2508.17481"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2508.17481"
      }
    ],
    "tags": [
      "Humanoid Robots",
      "Security Assessment",
      "Supply Chain",
      "Risk Prioritization"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 29,
    "title": "DevSecOps in Robotics",
    "year": "23 Mar 2020",
    "summary": "Advocates for integrating security practices into the robotics software lifecycle, outlining continuous integration, testing, and deployment controls tailored to ROS ecosystems.",
    "sources": [
      {
        "type": "paper",
        "title": "DevSecOps in Robotics",
        "url": "https://arxiv.org/abs/2003.10402"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2003.10402"
      }
    ],
    "tags": [
      "Robotics Security",
      "DevSecOps",
      "Secure Development",
      "Continuous Integration"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 30,
    "title": "Alurity: A Systematic Review on the Security of Robotic and Autonomous Systems",
    "year": "25 Mar 2022",
    "summary": "Surveys research on cyber-physical threats to robots and autonomous platforms, synthesizing mitigation strategies across networking, perception, and control layers.",
    "sources": [
      {
        "type": "paper",
        "title": "Alurity: A Systematic Review on the Security of Robotic and Autonomous Systems",
        "url": "https://arxiv.org/abs/2203.13874"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.13874"
      }
    ],
    "tags": [
      "Robotics Security",
      "Systematic Review",
      "Autonomous Systems",
      "Vulnerability Analysis"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 31,
    "title": "Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect",
    "year": "26 Nov 2020",
    "summary": "Demonstrates that modulated light sources can exploit rolling shutter readout to imprint invisible stripe patterns on captured images, enabling remote, human-imperceptible misclassification of ImageNet targets by commodity camera pipelines.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2011.13375"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2011.13375"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Rolling Shutter",
      "Optical Attack",
      "Invisible Perturbation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 32,
    "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
    "year": "10 Aug 2022",
    "summary": "Analyzes how pulsed, multi-color laser projections can spoof traffic light recognition systems in autonomous vehicles by overloading camera sensors, and studies countermeasures spanning sensing hardware, perception, and control layers.",
    "sources": [
      {
        "type": "paper",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/yan"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.usenix.org/system/files/sec22-yan.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Attack",
      "Traffic Infrastructure",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 33,
    "title": "Security Analysis of Camera-LiDAR Fusion Against Black-Box Attacks on Autonomous Vehicles",
    "year": "10 Aug 2022",
    "summary": "Evaluates the resilience of camera-LiDAR fusion stacks against black-box adversaries, presenting real-world spoofing attacks and hardening guidance for perception fusion used by autonomous cars.",
    "sources": [
      {
        "type": "paper",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/hallyburton"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.usenix.org/system/files/sec22-hallyburton.pdf"
      }
    ],
    "tags": [
      "Sensor Fusion",
      "Autonomous Driving",
      "LiDAR Spoofing",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 34,
    "title": "On the Realism of LiDAR Spoofing Attacks against Autonomous Driving Vehicle at High Speed and Long Distance",
    "year": "24 Feb 2025",
    "summary": "Shows experimentally that frequency-modulated LiDAR spoofing can introduce phantom obstacles against vehicles traveling at highway speeds and ranges, and analyzes mitigation strategies for automotive perception stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "NDSS 2025 paper",
        "url": "https://www.ndss-symposium.org/ndss-paper/on-the-realism-of-lidar-spoofing-attacks-against-autonomous-driving-vehicle-at-high-speed-and-long-distance/"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.ndss-symposium.org/wp-content/uploads/2025-628-paper.pdf"
      }
    ],
    "tags": [
      "LiDAR Spoofing",
      "Autonomous Driving",
      "Physical Adversarial",
      "Sensor Security"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 35,
    "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
    "year": "22 Oct 2023",
    "summary": "Introduces an optical dazzle attack that blinds CMOS rolling-shutter cameras under photopic conditions invisible to humans, causing downstream vision models to fail while keeping the light source covert.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2311.16118"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16118"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Attack",
      "Camera Sensor Spoofing",
      "Stealth"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 36,
    "title": "Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon",
    "year": "08 Mar 2022",
    "summary": "Crafts shadow-based perturbations that harness natural illumination to mislead traffic sign recognizers under black-box assumptions, demonstrating high success in simulation and real driving scenarios.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2203.03818"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.03818"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Shadow Attack",
      "Traffic Sign Spoofing",
      "Black-Box"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 37,
    "title": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study",
    "year": "14 Jun 2020",
    "summary": "Validates projector-based adversarial patterns that reliably misidentify subjects in face recognition systems across physical environments, quantifying success rates under varied lighting and motion.",
    "sources": [
      {
        "type": "paper",
        "title": "CVPR 2020 Workshop paper",
        "url": "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Nguyen_Adversarial_Light_Projection_Attacks_on_Face_Recognition_Systems_A_Feasibility_CVPRW_2020_paper.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Face Recognition",
      "Optical Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 38,
    "title": "ProjAttacker: A Configurable Physical Adversarial Attack for Face Recognition via Projector",
    "year": "19 Jun 2025",
    "summary": "Introduces a configurable framework for generating projector-based adversarial patterns that adapt to diverse camera poses and face models, improving stealth and transferability of physical face attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "CVPR 2025 paper",
        "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_ProjAttacker_A_Configurable_Physical_Adversarial_Attack_for_Face_Recognition_via_CVPR_2025_paper.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Face Recognition",
      "Configurable"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 39,
    "title": "Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach",
    "year": "01 Mar 2025",
    "summary": "Designs clothing-borne textures that transfer from simulation to real-world evaluations, degrading event-camera pedestrian detectors and revealing vulnerabilities in neuromorphic perception stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2503.00377"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2503.00377"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Event Cameras",
      "Pedestrian Detection",
      "Adversarial Clothing"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 40,
    "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
    "year": "07 Aug 2024",
    "summary": "Evaluates goal-hijacking visual prompt injections against LVLMs such as GPT-4V, introducing attack benchmarks and showing double-digit success rates that depend on character recognition and instruction following.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2408.03554"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2408.03554"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Prompt Injection",
      "Security Evaluation",
      "Goal Hijacking"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 41,
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "year": "17 May 2024",
    "summary": "Proposes SmoothVLM, a smoothing-based defense that counters adversarial patch prompt injections on VLMs, reducing attack success while preserving task fidelity across benchmark datasets.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2405.10529"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.10529"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Prompt Injection",
      "Defense",
      "Adversarial Patch"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 42,
    "title": "Prompt injection attacks on vision language models in oncology",
    "year": "01 Feb 2025",
    "summary": "Documents how imperceptible prompt injections embedded in medical imagery can coerce clinical VLMs into harmful recommendations, quantifying vulnerabilities across multiple proprietary models in oncology workflows.",
    "sources": [
      {
        "type": "paper",
        "title": "Nature Communications article",
        "url": "https://www.nature.com/articles/s41467-024-55631-x"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Healthcare AI",
      "Prompt Injection",
      "Risk Assessment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 43,
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "year": "28 Sep 2024",
    "summary": "Introduces TrojVLM, a backdoor that injects attacker-controlled text into VLM outputs while preserving semantic fidelity, demonstrating risks for image captioning and VQA pipelines.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2409.19232"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2409.19232"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Backdoor Attack",
      "Image Captioning",
      "Adversarial NLP"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 44,
    "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP",
    "year": "26 Nov 2023",
    "summary": "Develops BadCLIP, a trigger-aware prompt tuning method that implants persistent backdoors into CLIP without extensive data, achieving high attack success while retaining clean accuracy across datasets.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2311.16194"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16194"
      }
    ],
    "tags": [
      "CLIP",
      "Backdoor Attack",
      "Prompt Learning",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 45,
    "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
    "year": "05 Feb 2024",
    "summary": "Presents Shadowcast, a data poisoning pipeline that hides malicious perturbations in VLM training pairs to drive both label flipping and persuasive misinformation behaviors with minimal poison budget.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2402.06659"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.06659"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Data Poisoning",
      "Stealth Attack",
      "Misinformation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 46,
    "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
    "year": "13 Mar 2023",
    "summary": "Introduces ROCLIP, a robust CLIP training strategy that breaks poisoned image-caption associations via caption pools and augmentations, cutting targeted attack success while retaining downstream accuracy.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2303.06854"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.06854"
      }
    ],
    "tags": [
      "CLIP",
      "Defense",
      "Data Poisoning",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 47,
    "title": "Jailbreaking LLM-Controlled Robots",
    "year": "17 Oct 2024",
    "summary": "Introduces RoboPAIR, an automated jailbreak generator that coaxes LLM-driven robots into harmful actions across white-, gray-, and black-box setups, evidencing real-world safety risks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2410.13691"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2410.13691"
      },
      {
        "type": "website",
        "title": "Project site",
        "url": "https://robopair.org"
      }
    ],
    "tags": [
      "Robotics Security",
      "LLM Jailbreak",
      "Autonomous Systems",
      "Safety"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 48,
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "year": "12 Oct 2023",
    "summary": "Introduces Prompt Automatic Iterative Refinement (PAIR), a black-box jailbreak algorithm that efficiently crafts adversarial prompts for closed-source LLMs with fewer than twenty interactions.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2310.08419"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2310.08419"
      }
    ],
    "tags": [
      "LLM Security",
      "Jailbreak Attack",
      "Prompt Engineering",
      "Black-Box"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 49,
    "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
    "year": "18 Nov 2024",
    "summary": "Backdoors modular robotic policies by inserting poisoned VLM perception modules, demonstrating high-success physical attacks across UR3e manipulator tasks with stealthy trigger designs.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.11683"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.11683"
      }
    ],
    "tags": [
      "Robotics Security",
      "Backdoor Attack",
      "Vision-Language Models",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 50,
    "title": "Attacking Vision-Language Computer Agents via Pop-ups",
    "year": "04 Nov 2024",
    "summary": "Demonstrates that adversarial pop-up windows can reliably derail VLM-driven computer agents, cutting benchmark success rates nearly in half despite simple instruction-level defenses.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.02391"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.02391"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Agent Security",
      "Prompt Injection",
      "Human-Computer Interaction"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 51,
    "title": "The Translucent Patch: A Physical and Universal Attack on Object Detectors",
    "year": "23 Dec 2020",
    "summary": "Designs camera-mounted translucent patches that universally hide chosen object classes from state-of-the-art detectors while leaving other detections largely intact in autonomous driving benchmarks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2012.12528"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2012.12528"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Object Detection",
      "Universal Attack",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 52,
    "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
    "year": "30 Apr 2024",
    "summary": "Introduces thermal infrared triggers that plant backdoors in TIOD models, detailing temperature and material considerations for high-success attacks in both digital and field tests.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2404.19417"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2404.19417"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Backdoor Attack",
      "Thermal Imaging",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 53,
    "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
    "year": "16 May 2024",
    "summary": "Analyzes adversarial attacks on multimodal LLM visual grounding, proposing untargeted and targeted perturbations that misalign bounding boxes and benchmarking robustness across state-of-the-art models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2405.09981"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.09981"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Adversarial Attack",
      "Visual Grounding",
      "Robustness"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 54,
    "title": "GPT-4 Vision Prompt Injection: Risks, Examples & Defense",
    "year": "16 Oct 2023",
    "summary": "Survey article outlining how GPT-4V prompt injections manifest in practice, cataloging example exploits, and recommending operational mitigations for computer vision deployments.",
    "sources": [
      {
        "type": "article",
        "title": "Roboflow blog post",
        "url": "https://blog.roboflow.com/gpt-4-vision-prompt-injection/"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Prompt Injection",
      "Security Awareness",
      "Best Practices"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 55,
    "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
    "year": "01 Jun 2025",
    "summary": "Presents CAPAA, an optimization framework that aggregates gradients from multiple classifiers to craft projector-based adversarial light patterns resilient to pose changes and model swaps.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2506.00978"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.00978"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/ZhanLiQxQ/CAPAA"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Classifier Agnostic",
      "Robustness"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 56,
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "year": "18 Nov 2024",
    "summary": "Benchmarks untargeted and targeted attacks plus adversarial patches against VLA robots, revealing drastic drops in task success and highlighting spatially-informed threat objectives.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.13587"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.13587"
      }
    ],
    "tags": [
      "Robotics Security",
      "Vision-Language Models",
      "Adversarial Attack",
      "VLA"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 57,
    "title": "RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World",
    "year": "14 Jul 2023",
    "summary": "Uses reflected colored light patterns to craft stealthy adversarial perturbations effective under daylight, validated across datasets and physical setups without conspicuous markers.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2307.07653"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2307.07653"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Attack",
      "Stealth",
      "Reflected Light"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 58,
    "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
    "year": "10 Jul 2024",
    "summary": "Builds GhostStripe, a timing-controlled LED attack that creates invisible rolling-shutter stripes on traffic signs, achieving sustained misclassification of autonomous vehicle perception.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2407.07510"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.07510"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Rolling Shutter",
      "Traffic Sign Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 59,
    "title": "Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs",
    "year": "02 Jun 2022",
    "summary": "Optimizes low-cost laser spot parameters via genetic algorithms to conduct daytime-visible yet covert attacks that transfer across deep vision models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2206.01034"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2206.01034"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/ChengYinHu/AdvLS"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Attack",
      "Robustness",
      "Transferability"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 60,
    "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
    "year": "22 Mar 2023",
    "summary": "Surveys invasive and non-invasive optical physical attacks, categorizing techniques such as lasers, projectors, and reflective perturbations against deep vision models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2303.12249"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.12249"
      }
    ],
    "tags": [
      "Survey",
      "Physical Adversarial",
      "Optical Attack",
      "Computer Vision"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 61,
    "title": "Phantom of the ADAS: Securing Advanced Driver-Assistance Systems from Split-Second Phantom Attacks",
    "year": "09 Nov 2020",
    "summary": "Analyzes projection-based phantom imagery that tricks ADAS perception within milliseconds, and proposes hardware-software defenses tailored to production autonomous driving stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "ACM CCS 2020",
        "url": "https://dl.acm.org/doi/10.1145/3372297.3423359"
      }
    ],
    "tags": [
      "Autonomous Driving",
      "Sensor Spoofing",
      "Security Assessment",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 62,
    "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
    "year": "15 Feb 2024",
    "summary": "Characterizes how small instruction and perception perturbations degrade LLM/VLM robot controllers, offering perturbation strategies that expose 14–22% drops in task success.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2402.10340"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.10340"
      }
    ],
    "tags": [
      "Robotics Security",
      "LLM Vulnerability",
      "Vision-Language Models",
      "Robustness"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 63,
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "year": "03 Jun 2025",
    "summary": "Adapts LLM jailbreaking techniques to vision-language-action controllers, demonstrating that brief textual prompts can seize control authority over real robot platforms.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2506.03350"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.03350"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/eliotjones1/robogcg"
      }
    ],
    "tags": [
      "Robotics Security",
      "Vision-Language Models",
      "Adversarial Attack",
      "LLM Jailbreak"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 64,
    "title": "Latent Diffusion Models are Powerful Visual Anomaly Detectors",
    "year": "22 Jul 2022",
    "summary": "Demonstrates that latent diffusion models pretrained on generic imagery provide rich feature spaces for anomaly detection, achieving state-of-the-art accuracy on MVTec AD and VisA benchmarks without task-specific supervision.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2207.11569"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2207.11569"
      }
    ],
    "tags": [
      "Anomaly Detection",
      "Latent Diffusion",
      "Industrial Inspection",
      "Computer Vision"
    ],
    "last_reviewed": "24 Sep 2025"
  }
]
